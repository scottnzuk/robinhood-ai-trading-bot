# Decorator Integration Architecture: `@timeout_async` and `@safe_mps_op`

*Date: 2025-04-06*

---

## 1. Decorator Designs

### `timeout_async(seconds=30)`
- **Purpose:** Enforce max execution time on async functions.
- **Signature:**
  ```python
  def timeout_async(seconds: int = 30):
      def decorator(func):
          @functools.wraps(func)
          async def wrapper(*args, **kwargs):
              return await asyncio.wait_for(func(*args, **kwargs), timeout=seconds)
          return wrapper
      return decorator
  ```
- **Behavior:**
  - Cancels task if timeout exceeded.
  - Raises `asyncio.TimeoutError`.
  - Timeout configurable per use.
- **Edge Cases:**
  - Nested timeouts: innermost applies.
  - Propagates original exceptions if not timeout.

---

### `safe_mps_op`
- **Purpose:** Catch MPS-specific errors, fallback to CPU.
- **Signature:**
  ```python
  def safe_mps_op(fn):
      @functools.wraps(fn)
      def wrapper(*args, **kwargs):
          try:
              return fn(*args, **kwargs)
          except RuntimeError as e:
              if "MPS" in str(e):
                  # Log and fallback
                  ...
              else:
                  raise
      return wrapper
  ```
- **Async variant:** Wraps async functions similarly.
- **Behavior:**
  - On MPS error:
    - Log warning.
    - Convert all `torch.Tensor` on `"mps"` to `"cpu"`.
    - Retry function on CPU.
  - Else re-raise.
- **Edge Cases:**
  - Handles nested tensors.
  - Avoids fallback loops.

---

## 2. Integration Targets

| Module | Function(s) | Decorator(s) | Timeout (s) | Notes |
|---------|-------------|--------------|-------------|-------|
| `training_loop.py` | `main_training_loop` | `timeout_async`, `safe_mps_op` | 300 | Long training epochs |
| `main_loop.py` | `trading_loop`, `continual_learning_loop`, `monitoring_loop` | `timeout_async` | 60 | Prevent hangs |
| `rl_agents/a3c.py` | Policy/value updates | `safe_mps_op` | N/A | GPU ops |
| `feature_engineering.py` | Tensor conversions | `safe_mps_op` | N/A | Device-sensitive |
| `data_preprocessing.py` | Tensor ops | `safe_mps_op` | N/A | Device-sensitive |
| API calls (if async) | API fetch/send | `timeout_async` | 30 | Network timeouts |

---

## 3. Flow Diagram

```mermaid
flowchart TD
    subgraph Decorator Layer
        A1[Async Function Call]
        A2{@timeout_async}
        A3{@safe_mps_op}
    end
    subgraph Execution
        B1[Run Function]
        B2{MPS Error?}
        B3[Convert tensors to CPU]
        B4[Retry on CPU]
        B5{Timeout?}
        B6[Raise TimeoutError]
    end

    A1 --> A2 --> A3 --> B1
    B1 --> B2
    B2 -- No --> B5
    B2 -- Yes --> B3 --> B4 --> B5
    B5 -- No --> Done[Return Result]
    B5 -- Yes --> B6
```

---

## 4. Testing & Validation

- **Decorator Unit Tests:**
  - Timeout triggers correctly.
  - MPS error triggers fallback.
  - Normal execution unaffected.
- **Integration Tests:**
  - Simulate slow API/training to test timeout.
  - Inject MPS errors (mock or fault injection).
  - Verify fallback and recovery.
- **Regression Tests:**
  - Full trading loop.
  - RL agent training.
  - Data pipeline.

---

## 5. Rollback & Monitoring

- **Rollback:**
  - Decorators are non-invasive; can be removed easily.
  - Use feature flags/env vars to disable decorators if needed.
- **Monitoring:**
  - Log all timeouts and MPS fallbacks.
  - Count occurrences.
  - Alert on excessive failures/timeouts.

---

## 6. Implementation Notes

- Place decorators in `src/ai_trading_framework/mps_utils.py` or a new `decorators.py`.
- Import decorators explicitly in target modules.
- Use configurable timeout defaults.
- Document decorator usage inline.
- Update Memory Bank decision log upon integration.

---

*Generated by Roo Architect on 2025-04-06*